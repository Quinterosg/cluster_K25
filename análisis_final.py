# -*- coding: utf-8 -*-
"""Análisis_de_datos_Final _v.1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16qKx9buUBMFPfbsqrOagDgsx5L_r3YUm

## **Análisis exploratorio de datos de EA SPORT FC25**
"""

#Resumen: La base de datos tiene un formato csv,

"""# 1. Importar librerías"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns ## Gráficas Estadísticas

"""## 2.Importar datos



La primera forma es importarlos localmente
"""

datos = pd.read_csv("/content/Hojita_FC25 - all_players.csv")
datos.head()  #cabeza
#datos.tail()   cola

datos.info()

datos.describe().T

datos.shape #dimensión

# eliminar la columna indice
#datos.drop('indice', axis=1, inplace = True)   # Escribimos el nombre de la columna

"""Esto significa que contamos con 16161 muestras o samples y 57 columnas o variables

## 10 ligas con mayor cantidad de jugadores
"""

ligas = datos["League"].value_counts().head(10)

## Gráfica
plt.figure(figsize=(6,4))
ligas.plot(kind="bar", color = "blue") ## bar si la quiero vertical, barh si la quiero horizontal
plt.title("Ligas con mayor cantidad de jugadores")
plt.xlabel("Ligas")
plt.ylabel("Cantidad de jugadores")
plt.show()

"""# Distribución del "Overral"
"""

#Creamos un hsitograma
x = datos["OVR"]
bins_ = np.arange(40,100,1)

plt.figure(figsize=(6, 4))
sns.histplot(x = "OVR",
             data = datos,
             bins = bins_,
             kde = True,
             color = "g")
plt.title("Distribución del Overall")
plt.xlabel("Overall")
plt.show()

"""## Overral por posición"""

positions = datos.groupby("Position")["OVR"].median().sort_values().index

positions

## Diagrama de cajas (Boxplot)
sns.boxplot(x = "Position",           ##nombre de la variable en el conjuto de datos
           y = "OVR",                 ## nombre de la variable dependiente
           data = datos,              ##conjunto de datos
           order = positions,         ## variable creada anteriormente para los limites
           palette = "Accent")
plt.title(" Overral por posición")
plt.show()

"""# Matriz de correlación"""

#atributos de la matriz de correlación
corr_matriz =datos[["OVR", "PAC", "SHO", "PAS", "DRI", "DEF", "PHY"]].corr()
mask = np.triu(np.ones_like(corr_matriz, dtype=bool))
#corr_matriz = datos.corr()

 #Grafica
plt.figure(figsize=(8,6))
sns.heatmap(corr_matriz, annot=True, cmap="RdYlGn", linewidths=0.5)
#plt.title("Matriz de correlación")
plt.title("Matriz de correlación de los atributos de los jugadores")
plt.show()

import warnings
warnings.filterwarnings('ignore')
attributes = ['PAC', 'SHO', 'DRI', 'PHY', 'DEF', 'PAS']

fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(18, 15), sharey=True,)
fig.suptitle('Player attributes by position', fontsize=16)


axes = axes.flatten()


for i, attr in enumerate(attributes):
    order = datos.groupby('Position')[attr].median().sort_values().index

    sns.boxplot(x='Position', y=attr, data=datos, ax=axes[i], order=order, palette = 'PiYG')
    axes[i].set_title(f'{attr} rating by position')
    axes[i].set_ylabel(f'{attr}')
    axes[i].set_xlabel('Position')

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""# Clasificacion"""

##exportar el conjuntos de datos
x_ = pd.DataFrame (x_)
#Exportar el conjunto de datos locales
x_.to_csv("datos_locales_imputados.csv", index= False)

"""##Publicar datos en la web"""

### Publicar los datos en la web
url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRW8tZTTa9WTQIwih4N2ODY_NXG6EIh2UmqmIgdxaHSBFajBcHHkmIQeQKIH-yP5K3tHx9f0lFWXK-5/pub?output=csv'

data = pd.read_csv(url)
data.head()

data.describe().T

data.columns

columnas_objetivo = [        'Sprint Speed', 'Positioning', 'Finishing', 'Shot Power', 'Long Shots',
       'Volleys', 'Penalties', 'Vision', 'Crossing', 'Free Kick Accuracy',
       'Short Passing', 'Long Passing', 'Curve', 'Dribbling', 'Agility',
       'Balance', 'Reactions', 'Ball Control', 'Composure', 'Interceptions',
       'Heading Accuracy', 'Def Awareness', 'Standing Tackle',
       'Sliding Tackle', 'Jumping', 'Stamina', 'Strength', 'Aggression',
       'Weak foot', 'Skill moves', 'Age', 'GK Diving', 'GK Handling',
       'GK Kicking', 'GK Positioning', 'GK Reflexes', 'Preferred foot',
       'Nation', 'Position']
columnas = data.columns

for col in columnas:
  if col not in columnas_objetivo:
    data.drop(col, axis = 1, inplace = True)
  else:
    data[col] = data[col]

data.head()

data.isna().sum()

datos.shape #dimensión

"""# Imputacion de datos"""

# imputar datos faltantes para las categorias de GK cambiando NnN POR 0
gk_colums =["GK Diving","GK Handling","GK Kicking","GK Positioning","GK Reflexes"]

data[gk_colums] = data[gk_colums].fillna(0)

#Extraer datos
x = data.iloc[:,1:].values #traer todos los datos

x.shape

"""#codificar pie preferido"""

## codificar el pie preferido como variable numerica
from sklearn.preprocessing import LabelEncoder

#crear el codificador
le = LabelEncoder()

#le.fit()  dar a entender cual es el conjunto de datos
#le.transform()  cambiar segun lo que yo le doy a entender
#le.fit_transform() los dos anteriores unidos

#codificarla penultima columna (preferred foot)
x[:,-3] = le.fit_transform(x[:,-3])

x[:,-3]

df = pd.DataFrame(x) # data frame tipo de dato de pandas
for col in df.columns:
    # Convertir a numérico, forzando errores a NaN
    df[col] = pd.to_numeric(df[col], errors='coerce') #los errores los vuelvo datos faltantes

#df = pd.DataFrame(x)
#for col in df.columns:
    # Convertir a numérico, forzando errores a NaN
 #   df[col] = pd.to_numeric(df[col], errors='coerce')

data.iloc[:,-3]

x = df.values
x

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='median') # cuando empieza con mayuscula de denomina "clase" SimpleImputer
imputer.fit(x)
x = imputer.transform(x)
x

x_ =  x ##arreglo de numpy x[:,:-1]     estamos eligiendo todos los datos excepto el ultimo

x_ = pd.DataFrame(x_)
#Exportar el conjunto de datos locales
x_.to_csv("datos_locales_imputados.csv", index = False)

# Añadir los clusters al DataFrame original
#data['Cluster'] = clusters  #crear una nueva columna con la info de los clusters

#print(data[['Name',  'Cluster']].head())  # Muestra los primeros resultados

"""# Visualizacion

##PCA   : para restringir o disminuir la distancia
"""

from sklearn.decomposition import PCA #pca:  analisis de componentes principales
# Crear un objeto PCA y ajustar los datos
pca = PCA(n_components=3)  # Queremos las tres primeras componentes principales
x_pca = pca.fit_transform(x_)

varianza = pca.explained_variance_ratio_
print(sum(varianza))

x_pca

"""#K_means

##Método de codo
"""

from sklearn.cluster import KMeans
#
wcss = []

for k in range(1, 11):
    kpp = KMeans(n_clusters=k , #para el kmeans++
                 random_state=42, n_init = "auto")
    kpp.fit(x_)
    wcss.append(kpp.inertia_)
plt.figure()
plt.plot(range(1, 11), wcss, "o--", color = "green")
plt.title('Método del codo en k-means')
plt.xlabel('i_Número de clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.grid(linestyle = "--")
plt.show()

"""## Implementación del algoritmo sklearn"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=4, random_state=42) #son los hiper parametros que utilizaremos
clusters_kmeans = kmeans.fit_predict(x_) #variable que usea fit_predic para organizar, entender, otorgar una etique a todos los datos

"""### ver centroides"""

## ver centroides
# Calcula los centros de los clústeres usando kmeans
centroides_kmeans = kmeans.cluster_centers_


# Aplica la misma transformación PCA a los centros de los clústeres
centroides_kmeans = pca.transform(centroides_kmeans)

# Imprime los centros de los clústeres transformados
centroides_kmeans

centroides_kmeans_pca =  centroides_kmeans.copy()
centroides_kmeans_pca

"""# Visualizacion:

##Funciones para graficar
"""

##para reutilizar el clusters solo creamos una funcion con esta informacion que reciba como argumento "clusters"
def graficar_clusters_pca2D(metodo, clusters, centroides):
    #Crearemos una grafica de dispersion en 2D con las dos primeras principales componentes de los datos usados
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=x_pca[:, 0], y=x_pca[:, 1],
                hue = clusters, ##### para cambiar la visualizacion se cambia el hue y el nombre kmeans
                palette='pastel', alpha=0.6, s=100)
    plt.scatter(centroides_kmeans[:, 0], centroides_kmeans[:, 1], c ='black', s = 100, label = 'Centroides')
    plt.title('Visualización 2D de la Clusterización (PCA)', fontsize=16)
    plt.xlabel('Componente Principal 1')
    plt.ylabel('Componente Principal 2')
    plt.grid(True)
    plt.grid(linestyle = "-", alpha = 0.5)
    plt.legend(title='Cluster')
    plt.show()

#ahora definiremos la funcion para reutilizar los clusters pero esta vez para grafica 3D
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np
def graficar_clusters_pca_en3D(metodo, clusters):
          # Crear un gráfico de dispersión 3D usando las tres primeras componentes
          fig = plt.figure(figsize=(15, 8))
          ax = fig.add_subplot(111, projection='3d')

          # Graficar los puntos
          scatter = ax.scatter(x_pca[:, 0], x_pca[:, 1], x_pca[:, 2], c=clusters ,
                               cmap='viridis', s=100,
                               edgecolor = 'white', alpha = 0.6)

          # Etiquetas de los ejes y título del grafico
          ax.set_title(f'Visualización 3D de la Clusterización (PCA), {metodo}', fontsize=16)
          ax.set_xlabel('Componente Principal 1')
          ax.set_ylabel('Componente Principal 2')
          ax.set_zlabel('Componente Principal 3')

          # Crear la leyenda
          legenda_ = ax.legend(*scatter.legend_elements(), title='Cluster')
          ax.add_artist(legenda_)
          #unique_clusters = np.unique(clusters)
          #colors = scatter.get_array()

          # Añadir la leyenda manualmente
          #handles = []


          # Mostrar el gráfico
          plt.show()

"""##Gráficas"""

#Llamamos a la funcion para la grafica en 2D
kmeans_2D = graficar_clusters_pca2D('KMeans', clusters_kmeans, centroides_kmeans)
kmeans_2D

#Llamamos a la funcion para la grafica pero esta vez en 3D
kmeans_3D = graficar_clusters_pca_en3D('K-Means', clusters_kmeans)
kmeans_3D

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

"""...

# K-means distacia euclidiana
"""

import random
# Definimos la distancia Euclideana
def distancia_euclideana(x, c):
  return np.sqrt(np.sum((x - c) ** 2))

# Paso 1: Número de clusters
k = 4  # Por heurística de los datos y método del codo

# Paso 2: Elección de centroides
## Centroides
dimension = x_.shape[0]  # Número de jugadores (filas del DataFrame)

centroids = x_.iloc[random.sample(range(dimension), k)].values
print(data.iloc[centroids[:,0],0])
print(data.iloc[centroids[:,1],0])
print(data.iloc[centroids[:,2],0])

### Almacenamiento de etiquetas
clusters_manuales = np.zeros(dimension)  ## vector nulo para almacenar etiquetas
### Almacenamiento de distancias
distancias = np.zeros((dimension, k))     ## Matriz nula para almacenar distancias

# Paso 3: Implementación del método y criterios de convergencia
tol = 1e-6
error = 100

# Repetir hasta que los centroides dejen de moverse significativamente
while error > tol:
    # Asignación de puntos a los clusters más cercanos
    for i in range(dimension):  ## Iterar sobre las filas. La i representa jugador
        for j in range(k):      ## Iterar sobre las columnas. La j representa centroide
            distancias[i, j] = distancia_euclideana(x_.iloc[i].values, centroids[j])
        clusters_manuales[i] = np.argmin(distancias[i])

    # Almacenar los centroides previos antes de actualizarlos
    centroids_prev = np.copy(centroids) ## Creación variable auxiliar para comparar con los nuevos centroides

    # Actualizar centroides
    for l in range(k):  ### Iteramos sobre los clusters (k = 4)
        puntos_cluster = x_[clusters_manuales == l] ### Estamos tomando todos los puntos que tengan la etiqueta L

        # Si el cluster no está vacío, recalcula el centroide
        if len(puntos_cluster) > 0:
            centroids[l] = np.mean(puntos_cluster, axis=0)  ### Promedia los puntos que pertenecen al cluster, columna por columna
        else:
            print(f"Cluster {l} vacío, reasignando centroide aleatoriamente")
            centroids[l] = x_.iloc[np.random.choice(dimension)].values

    # Calcular el error como el cambio promedio en los centroides
    error = np.mean([distancia_euclideana(centroids[l], centroids_prev[l]) for l in range(k)])
    print(error)

clusters_manuales = clusters_manuales.astype(int)

"""# K-means distancia Mahalanobis"""

import random
from scipy.spatial import distance

cov = np.cov(x_.T)
cov_inv = np.linalg.inv(cov)

### Definiremos la distancia de Mahalanobis
def distancia_mahalanobis(x, c):
  return distance.mahalanobis(x, c, cov_inv)



# damos inicializacion a las variables
k = 4  #la cantidad de clusters
## Centroides
dimension = x_.shape[0]  # Número de jugadores (filas del DataFrame)
# Use iloc to select rows by their integer location
centroids_mahalanobis = x_.iloc[random.sample(range(dimension), k)].values
print(data.iloc[centroids_mahalanobis[:,0],0])
print(data.iloc[centroids_mahalanobis[:,1],0])
print(data.iloc[centroids_mahalanobis[:,2],0])

### Almacenamiento de etiquetas
clusters_manuales_mahalanobis = np.zeros(dimension)  ## vector nulo para almacenar etiquetas
### Almacenamiento de distancias
distancias = np.zeros((dimension, k))     ## Matriz nula para almacenar distancias

# Paso 3: Implementación del método y criterios de convergencia
tol = 1e-6
error = 100

# Repetir hasta que los centroides dejen de moverse significativamente
while error > tol:
    # Asignación de puntos a los clusters más cercanos
    for i in range(dimension):  ## Iterar sobre las filas. La i representa jugador
        for j in range(k):      ## Iterar sobre las columnas. La j representa centroide
            distancias[i, j] = distancia_mahalanobis(x_.iloc[i].values, centroids_mahalanobis[j])
        clusters_manuales_mahalanobis[i] = np.argmin(distancias[i])

    # Almacenar los centroides previos antes de actualizarlos
    centroids_prev = np.copy(centroids_mahalanobis) ## Creación variable auxiliar para comparar con los nuevos centroides

    # Actualizar centroides
    for l in range(k):  ### Iteramos sobre los clusters (k = 4)
        puntos_cluster = x_[clusters_manuales_mahalanobis == l] ### Estamos tomando todos los puntos que tengan la etiqueta L

        # Si el cluster no está vacío, recalcula el centroide
        if len(puntos_cluster) > 0:
            centroids_mahalanobis[l] = np.mean(puntos_cluster, axis=0)  ### Promedia los puntos que pertenecen al cluster, columna por columna
        else:
            print(f"Cluster {l} vacío, reasignando centroide aleatoriamente")
            centroids_mahalanobis[l] = x_.iloc[np.random.choice(dimension)].values

    # Calcular el error como el cambio promedio en los centroides
    error = np.mean([distancia_mahalanobis(centroids_mahalanobis[l], centroids_prev[l]) for l in range(k)])
    print(error)

from scipy.spatial import distance


from scipy.spatial import distance


# con esta linea vamos a calcular la matriz de covarianza y su inversa para mahalanobis
cov = np.cov(x.T)
cov_inv = np.linalg.inv(cov)

# Definimos la función para la distancia de Mahalanobis
def distancia_mahalanobis(x, c):
    return distance.mahalanobis(x, c, cov_inv)


## le damos valor de inicio a kmeans ++ para los centroides
def inicializar_centroides_kmeans_pp(x , k):      #inicializar
#inicializamos los centroides
    centroides = []

    centroides.append(x.values[np.random.choice(range(x.shape[0]))])

    for _ in range(1, k):
    #esta linea siver para calcular la distancia minima del centroide mas cercarno
      distancias_minimas = np.array([min(np.linalg.norm(x - c) for c in centroids) for x in x.values]) # Access the underlying NumPy array using `.values`

      #selecionamos el siguiente centroide el cual tiene una probabilidad o variabilidad que se proporciona con la distancia al cuadrado
      prob_distancias = distancias_minimas ** 2
      prob_distancias /= prob_distancias.sum()

    #finalmente evaluamos el ultimo centroide "codido del profe porque no llegamos a la manera de implementarlo por nosotros mismos"
      siguiente_centroide_x = np.random.choice(range(x.shape[0]), p=prob_distancias)

      centroids.append(x.values[siguiente_centroide_x])
    return np.array(centroids)


k = 4

# dimensiones y variables
dimension = x_.shape[0]
clusters_manuales_mahalanobis = np.zeros(dimension)
distancias = np.zeros((dimension, k))

# inicializamos los centroides de k_means ++
centroids_mahalanobis = inicializar_centroides_kmeans_pp(x_ , k)


#tolerancia de error
tol = 1e-6
error = 10

#repetir hasta que los centroides dejen de moverse significativamente
while error > tol:
  #asignamos puntos a los clusters que esten mas cercanos
  for i in range(dimension):
    for j in range(k):
      distancias[i, j] = distancia_mahalanobis(x_[i], centroids_mahalanobis[j])
      clusters_manuales_mahalanobis[i] = np.argmin(distancias[i])
    #almacenar los centroides
  centroids_prev_mahalanobis = np.copy(centroids_mahalanobis)

   #actualizar centroides
  for l in range(k):
    puntos_cluster = x_[clusters_manuales_mahalanobis == l]

    #si el cluster no esta vacio recalcula el centroide
    if len(puntos_cluster) > 0:
      centroids_mahalanobis[l] = np.mean(puntos_cluster, axis = 0)
    else:
      print(f"Cluster {l} vacio, reasignando centroide aleatoriamente")
      centroids_mahalanobis[l] = x_[np.random.choice(dimension)]
    #calcular el error como el cambio de promedo de los centroides
    error = np.mean([distancia_euclideana(centroids_mahalanobis[l], centroids_prev_mahalanobis[l]) for l in range(k)])

  print(error)

def inicializar_centroides_kmeans_pp(x, k):
    # Inicializa centroides como una lista vacía
    centroids = []  # Cambia esta línea
    # Accede al array NumPy subyacente usando `.values` y luego selecciona una fila aleatoria
    centroids.append(x.values[np.random.choice(range(x.shape[0]))])

    for _ in range(1, k):
        # esta línea sirve para calcular la distancia mínima del centroide más cercano
        distancias_minimas = np.array([min(np.linalg.norm(x - c) for c in centroids) for x in x.values])  # Accede al array NumPy subyacente usando `.values`

        # seleccionamos el siguiente centroide el cual tiene una probabilidad o variabilidad que se proporciona con la distancia al cuadrado
        prob_distancias = distancias_minimas ** 2
        prob_distancias /= prob_distancias.sum()

        # finalmente evaluamos el último centroide "código del profe porque no llegamos a la manera de implementarlo por nosotros mismos"
        siguiente_centroide_x = np.random.choice(range(x.shape[0]), p=prob_distancias)
        # Accede al array NumPy subyacente usando `.values` y luego selecciona la fila
        centroids.append(x.values[siguiente_centroide_x])
    return np.array(centroids)  # Convierte a array NumPy al final

import random

#clusters_manuales_maha = clusters_manuales_maha.astype(int)

"""# k-means distancia *L1* (Distancia de Manhattan)"""

#Distancia de Manhattan

def distancia_L1(x, c):

    return  np.sum(np.abs(x - c))# Suma de las diferencias absolutas



k = 4

# dimensiones y variables
dimension = x_.shape[0]
clusters_manuales_L1 = np.zeros(dimension)
distancias = np.zeros((dimension, k))

# inicializamos los centroides de k_means ++
centroids_L1 = x_.iloc[random.sample(range(dimension), k )].values

#tolerancia de error
tol = 1e-6
error = 10

#repetir hasta que los centroides dejen de moverse significativamente
while error > tol:
  #asignamos puntos a los clusters que esten mas cercanos
  for i in range(dimension):
    for j in range(k):
      distancias[i, j] = distancia_L1(x_.iloc[i].values, centroids_L1[j])
    clusters_manuales_L1[i] = np.argmin(distancias[i])
    #almacenar los centroides
  centroids_prev_L1 = np.copy(centroids_L1)

   #actualizar centroides
  for l in range(k):
    puntos_cluster = x_[clusters_manuales_L1 == l]

    #si el cluster no esta vacio recalcula el centroide
    if len(puntos_cluster) > 0:
      centroids_L1[l] = np.mean(puntos_cluster, axis = 0)
    else:
      print(f"Cluster {l} vacio, reasignando centroide aleatoriamente")
      centroids_L1[l] = x_[np.random.choice(dimension)]
    #calcular el error como el cambio de promedo de los centroides
    error = np.mean([distancia_euclideana(centroids_L1[l], centroids_prev_L1[l]) for l in range(k)])
  print(error)

clusters_manuales_L1 = clusters_manuales_L1.astype(int)

"""# **Gráficas de clusternizacion**

## k-means
"""

centroides_kmeans_pca = pca.transform(centroides_kmeans)
centroides_kmeans_pca

kmeans_2D = graficar_clusters_pca2D('KMeans', clusters_kmeans, centroides_kmeans)
kmeans_2D



"""##K-means ++"""



"""##k-means Euclideana"""

centroids = np.array(centroids)  # se tienen en cuenta 45 dimensiones
centroids = pca.transform(centroids) ## se usan 3 dimensiones

manual_2D = distancia_euclideana( clusters_manuales, centroids)
manual_2D

manual_3D = graficar_clusters_pca_en3D('K-means_manuales', clusters_manuales)
manual_3D